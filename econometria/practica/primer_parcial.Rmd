---
title: "Primer Parcial"
output:
  html_notebook: default
  word_document: default
  html_document:
    df_print: paged
---

## TeorÃ­a

### Punto 1
Un investigador esta interesado en estimar ğ¸[ğ‘Œ]. Sin embargo, una investigadora le sugiere
que seria mejor estimar ğ¸[ğ‘Œ|ğ‘‹]. Explque la diferencia de cada una de las metodologÃ­as y
grafique.

El valor esperado de Y es la esperanza matemÃ¡tica, el promedio, sin tener en cuenta ninguna
otra variable mas que los valores de Yi. En cambio ğ¸[ğ‘Œ|ğ‘‹] es la esperanza dado x, es decir, toma
en cuenta la relaciÃ³n que tiene Y con X, en nuestro caso para la funciÃ³n de regresiÃ³n muestral
$$
(*) Y=\beta_0+\beta_1 x+u   
$$
Sea Y=Consumo y X=Salario
En el siguiente grÃ¡fico se ve que el consumo tiene una relaciÃ³n con el salario y el valor esperado del consumo en relaciÃ³n con el salario pasa por la recta (*).
En cambio el promedio de consumo es 8

```{r}
x<-c(1,5,8,10,11,14)
y<-c(4,6,7,8,9,14)
datos=data.frame(x,y)
regressor=lm(y~x,data = datos)
plot(x,y)
abline(regressor)
abline(h=mean(y),col="blue")
```
### Punto 2)
Â¿CuÃ¡ndo deberÃ­a usar cada una de las fÃ³rmulas? Explique detalladamente y comente en que se
diferencian.

$$
(1)\hat\beta=\frac{\sum X*Y}{\sum X^2}
$$
$$
(2)\hat\beta=\frac{\sum(X - \bar X)(Y-\bar Y)}{\sum(X-\bar X)^2}
$$ 
La fÃ³rmula (1) se refiere a una regresion lineal por el orgin.
La fÃ³rmula (2) corresponde a la regresiÃ³n lineal cuando tiene interceptor.
El  *beta*  corresponde al calculo de la pendiente, en la formula  (1) se utiliza cuando el modelo al
que se quiere aproximar es una regresion por el origen. La diferencia con (2) es, como se ve,
que no esta centrada. Normalmente, salvo que exista una buena explicaciÃ³n, se
debe utilizar la formula (2)



### Punto 3
Desarrolle y comente la veracidad de la siguiente frase: â€œtodas variables con relaciÃ³n
estadÃ­stica tienen su correlaciÃ³n causalâ€.


Falso. CorrelaciÃ³n (la relaciÃ³n estadÃ­stica) no implica causalidad, por ejemplo puedo tomar la
variable libros comprados y tazas de leche llenas a la mitad en los bares de buenos aires,
correr una regresiÃ³n lineal, ver que R^2 es 0,80 y el p-value da 0.0001. Esto determina que
estÃ¡n correlacionados. Ahora bien, cuÃ¡l es la relaciÃ³n causal? A menos que los libros solo se
vendan (o la mayorÃ­a de ellos se compran) en bares donde llenan a la mitad la tazas de leche
difÃ­cilmente exista una relaciÃ³n causal.
Este ejemplo muestra que, ademÃ¡s de tener una relaciÃ³n estadÃ­stica, debe existir un vector que
vincule A con B de manera causal.


## Punto 4
4) Explique puntualmente que diferencia hay entre ğ›½^ y ğ›½. Â¿CuÃ¡l es el fijo y cual el aleatorio?
Â¿Por quÃ©? Relacione con insesgadez, eficiencia y consistencia.

 ğ›½^  (beta sombrero) es el estimador de beta que es el parÃ¡metro. Aca ğ›½^  es el
aleatorio porque se usa con la muestra aleatoria y el parÃ¡metro es el fijo.

Insesgadez, eficiencia y consistencia son las 3 caracterÃ­sticas de un estimador insesgado. 
Sicumple con esas caracterisitcas se puede decir que es un buen estimador, si es el de menor
varianza, serÃ¡ MELI (en este caso que es una regresiÃ³n lineal).

## PrÃ¡ctica
### Punto 1

#### Punto A

```{r}
library(stargazer)
vectorX_punto_1<-rnorm(1000,1,10)
vectorU<-rnorm(1000,0,2)
#Tomamos b0=3 y b1=5
vectorY<-3+5*vectorX_punto_1+vectorU # aca creo la fucion y le sumo el vector directamente.
#Nube de puntos del vector y,x
plot(vectorX_punto_1,vectorY)
# Los puntos parecen seguir una funcion lineal
```

#### Punto B y C y d
```{r}
modelo_1<-lm(vectorY~vectorX_punto_1,data = datos)
plot(vectorX_punto_1,vectorY)
abline(modelo_1)
modelo_2<-lm(vectorY~0+vectorX_punto_1,data = datos)
abline(modelo_2)
#stargazer(modelo_1,type="text")
#stargazer(modelo_2,type="text")
```
Si se amplÃ­a se puede ver que son muy parecidas las rectas, lo que condice con
que las pendientes son muy parecidas tambiÃ©n.

```{r}
stargazer(modelo_1,modelo_2,type="text")

```
El modelo 1 es una regresiÃ³n de dos parÃ¡metros y el modelo 1 es una regresiÃ³n por el origen.
Se puede ver que en el modelo 2 no hay constante, ambos tiene un coeficiente de
determinaciÃ³n muy parecido. La pendiente es muy similar en ambos 5.004 y 5.028 y ambos
modelos son significativos al 10%, al 5% y al 1%. Para la regresiÃ³n lineal con dos parÃ¡metros
podemos decir que el 99,8% de las variaciones son explicadas por X y el interceptor es
significativo al 10%, 5% y 1%.


#### Punto e
Si agregamos un vector con un conjunto de puntos con mayor desvÃ­o Y se va a volver mÃ¡s
disperso, la recta ya no ajusta muy bien.

### Punto 2
Una empresa encuestÃ³ a la poblaciÃ³n para obtener que impacto tiene el ingreso en el consumo
de su producto. 
$$
  consumo=\beta_1+\beta_2*Salario+u_i
$$
Interpretamos los parametros $$\beta_1=145$$
eso significa que cuando el salario es 0 el consumo es 145. 
El parÃ¡metro: $$ \beta_1=0.63$$
es la pendiente, significa que por cada incremento en una unidad adicional del salario el consumo se incrementa en .63 unidades.

ğ¶ğ‘œğ‘›ğ‘ ğ‘¢ğ‘šğ‘œi5 = ğ¶ğ‘œğ‘›ğ‘ ğ‘¢ğ‘šğ‘œi âˆ— 5. Entiendo que en este punto esta cambiando la escala esta
multiplicando por 5 todos los valores del consumo en este caso al multiplicar todos los
valores de Y (consumo) por 5 se modifica b1 y b2 b1*=5*b1 y b2*=5*b2.


